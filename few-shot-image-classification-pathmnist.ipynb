{"cells":[{"cell_type":"markdown","metadata":{"id":"x_bBRt3U7Krp","pycharm":{"name":"#%% md\n"}},"source":["## Discovering Prototypical Networks\n","First, let's install the [tutorial GitHub repository](https://github.com/sicara/easy-few-shot-learning) and import some packages. If you're on Colab right now, you should also check that you're using a GPU (Edit > Notebook settings)."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:00:32.630587Z","iopub.status.busy":"2022-12-04T19:00:32.630184Z","iopub.status.idle":"2022-12-04T19:00:47.220885Z","shell.execute_reply":"2022-12-04T19:00:47.219565Z","shell.execute_reply.started":"2022-12-04T19:00:32.63047Z"},"id":"1_tS8L5S9OTY","outputId":"5036b338-40c2-46c7-efdd-3b7d2f1118d8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting easyfsl\n","  Obtaining dependency information for easyfsl from https://files.pythonhosted.org/packages/e1/47/fa188980a02084661080fc07ac75150c45537308e769803fcf18653165fa/easyfsl-1.5.0-py3-none-any.whl.metadata\n","  Downloading easyfsl-1.5.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from easyfsl) (3.7.1)\n","Requirement already satisfied: pandas>=1.5.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from easyfsl) (1.5.3)\n","Requirement already satisfied: torch>=1.5.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from easyfsl) (2.1.1)\n","Requirement already satisfied: torchvision>=0.7.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from easyfsl) (0.16.1)\n","Requirement already satisfied: tqdm>=4.1.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from easyfsl) (4.65.0)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.0.5)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (4.25.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (23.0)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->easyfsl) (2022.7)\n","Requirement already satisfied: filelock in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torch>=1.5.0->easyfsl) (3.9.0)\n","Requirement already satisfied: typing-extensions in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torch>=1.5.0->easyfsl) (4.7.1)\n","Requirement already satisfied: sympy in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torch>=1.5.0->easyfsl) (1.11.1)\n","Requirement already satisfied: networkx in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torch>=1.5.0->easyfsl) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torch>=1.5.0->easyfsl) (3.1.2)\n","Requirement already satisfied: fsspec in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torch>=1.5.0->easyfsl) (2023.3.0)\n","Requirement already satisfied: requests in c:\\users\\preet\\anaconda3\\lib\\site-packages (from torchvision>=0.7.0->easyfsl) (2.31.0)\n","Requirement already satisfied: colorama in c:\\users\\preet\\anaconda3\\lib\\site-packages (from tqdm>=4.1.0->easyfsl) (0.4.6)\n","Requirement already satisfied: six>=1.5 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->easyfsl) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.5.0->easyfsl) (2.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from requests->torchvision>=0.7.0->easyfsl) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from sympy->torch>=1.5.0->easyfsl) (1.3.0)\n","Downloading easyfsl-1.5.0-py3-none-any.whl (72 kB)\n","   ---------------------------------------- 0.0/72.8 kB ? eta -:--:--\n","   ---------------------------------------  71.7/72.8 kB 2.0 MB/s eta 0:00:01\n","   ---------------------------------------- 72.8/72.8 kB 1.3 MB/s eta 0:00:00\n","Installing collected packages: easyfsl\n","Successfully installed easyfsl-1.5.0\n"]}],"source":["!pip install easyfsl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:00:47.229652Z","iopub.status.busy":"2022-12-04T19:00:47.227197Z","iopub.status.idle":"2022-12-04T19:00:51.419028Z","shell.execute_reply":"2022-12-04T19:00:51.418014Z","shell.execute_reply.started":"2022-12-04T19:00:47.229607Z"},"id":"gD5jDtGZ7Krp","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\preet\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From c:\\Users\\preet\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"]}],"source":["import torch\n","import numpy as np\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import Food101, Omniglot\n","from torchvision.models import resnet18\n","from tqdm import tqdm\n","\n","from easyfsl.samplers import TaskSampler\n","from easyfsl.utils import plot_images, sliding_average\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","import torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import medmnist\n","from medmnist import INFO, Evaluator\n","from medmnist import PathMNIST"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_size = 224\n","\n","data_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[.5], std=[.5])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set = PathMNIST(root='pathmnist_data', split='train', transform=data_transform, size=224, mmap_mode='r' ,download=True)\n","val_set = PathMNIST(root='pathmnist_data', split='val', transform=data_transform, size=224, mmap_mode='r' ,download=True)\n","test_set = PathMNIST(root='pathmnist_data', split='test', transform=data_transform, size=224, mmap_mode='r' ,download=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-12-04T19:00:51.421469Z","iopub.status.busy":"2022-12-04T19:00:51.420756Z","iopub.status.idle":"2022-12-04T19:05:49.388705Z","shell.execute_reply":"2022-12-04T19:05:49.384018Z","shell.execute_reply.started":"2022-12-04T19:00:51.421426Z"},"id":"OrUCQ7AslpFO","outputId":"5f78da84-6e2d-4fba-a665-de858c187a6e","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["# image_size = 224\n","\n","# train_set = Food101(\n","#     root=\"./data\",\n","#     transform=transforms.Compose(\n","#         [\n","#             # transforms.Grayscale(num_output_channels=3),\n","#             transforms.RandomResizedCrop(image_size),\n","#             transforms.RandomHorizontalFlip(),\n","#             transforms.ToTensor(),\n","#         ]\n","#     ),\n","#     download=True,\n","# )\n","# test_set = Food101(\n","#     root=\"./data\",\n","#     transform=transforms.Compose(\n","#         [\n","#             # Omniglot images have 1 channel, but our model will expect 3-channel images\n","#             # transforms.Grayscale(num_output_channels=3),\n","#             transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n","#             transforms.CenterCrop(image_size),\n","#             transforms.ToTensor(),\n","#         ]\n","#     ),\n","#     download=True,\n","# )\n","train_set.get_labels = lambda: train_set._labels\n","test_set.idx_to_class = {idx: cl for cl, idx in test_set.class_to_idx.items()}\n"]},{"cell_type":"markdown","metadata":{"id":"7HVUQX1F7Krq","pycharm":{"name":"#%% md\n"}},"source":["Let's take some time to grasp what few-shot classification is. Simply put, in a few-shot classification task, you have a labeled support set (which kind of acts\n","like a catalog) and query set. For each image of the query set, we want to predict a label from the\n","labels present in the support set. A few-shot classification model has to use the information from the\n","support set in order to classify query images. We say *few-shot* when the support set contains very\n","few images for each label (typically less than 10). The figure below shows a 3-way 2-shots classification task. \"3-way\" means \"3 different classes\" and \"2-shots\" means \"2 examples per class\".\n","We expect a model that has never seen any Saint-Bernard, Pug or Labrador during its training to successfully\n","predict the query labels. The support set is the only information that the model has regarding what a Saint-Bernard,\n","a Pug or a Labrador can be.\n","\n","![few-shot classification task](https://images.ctfassets.net/be04ylp8y0qc/bZhboqYXfYeW4I88xmMNv/7c5efdc368206feaad045c674b1ced95/1_AteD0yXLkQ1BbjQTB3Ytwg.png?fm=webp)\n","\n","Most few-shot classification methods are *metric-based*. It works in two phases : 1) they use a CNN to project both\n","support and query images into a feature space, and 2) they classify query images by comparing them to support images.\n","If, in the feature space, an image is closer to pugs than it is to labradors and Saint-Bernards, we will guess that\n","it's a pug.\n","\n","From there, we have two challenges :\n","\n","1. Find the good feature space. This is what convolutional networks are for. A CNN is basically a function that takes an image as input and outputs a representation (or *embedding*) of this image in a given feature space. The challenge here is to have a CNN that will\n","project images of the same class into representations that are close to each other, even if it has not been trained\n","on objects of this class.\n","2. Find a good way to compare the representations in the feature space. This is the job of Prototypical Networks.\n","\n","\n","![Prototypical classification](https://images.ctfassets.net/be04ylp8y0qc/45M9UcUp6KnzwDaBHeGZb7/bb2dcda5942ee7320600125ac2310af6/0_M0GSRZri859fGo48.png?fm=webp)\n","\n","From the support set, Prototypical Networks compute a prototype for each class, which is the mean of all embeddings\n","of support images from this class. Then, each query is simply classified as the nearest prototype in the feature space,\n","with respect to euclidean distance.\n","\n","If you want to learn more about how this works, I explain it\n","[there](https://www.sicara.ai/blog/2019-07-30-image-classification-few-shot-meta-learning-5fd736a6c54d2).\n","But now, let's get to coding.\n","In the code below (modified from [this](https://github.com/sicara/easy-few-shot-learning/blob/master/easyfsl/methods/prototypical_networks.py)), we simply define Prototypical Networks as a torch module, with a `forward()` method.\n","You may notice 2 things.\n","\n","1. We initiate `PrototypicalNetworks` with a *backbone*. This is the feature extractor we were talking about.\n","Here, we use as backbone a ResNet18 pretrained on ImageNet, with its head chopped off and replaced by a `Flatten`\n","layer. The output of the backbone, for an input image, will be a 512-dimensional feature vector.\n","2. The forward method doesn't only take one input tensor, but 3: in order to predict the labels of query images,\n","we also need support images and labels as inputs of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:05:49.402669Z","iopub.status.busy":"2022-12-04T19:05:49.400387Z","iopub.status.idle":"2022-12-04T19:06:13.109345Z","shell.execute_reply":"2022-12-04T19:06:13.108206Z","shell.execute_reply.started":"2022-12-04T19:05:49.402622Z"},"id":"iCRwLATr7Krr","outputId":"3be1f584-aaa2-4cd8-adc4-b034e97beb5b","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["class PrototypicalNetworks(nn.Module):\n","    def __init__(self, backbone: nn.Module):\n","        super(PrototypicalNetworks, self).__init__()\n","        self.backbone = backbone\n","\n","    def forward(\n","        self,\n","        support_images: torch.Tensor,\n","        support_labels: torch.Tensor,\n","        query_images: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Predict query labels using labeled support images.\n","        \"\"\"\n","        # Extract the features of support and query images\n","        z_support = self.backbone.forward(support_images)\n","        z_query = self.backbone.forward(query_images)\n","\n","        # Infer the number of different classes from the labels of the support set\n","        n_way = len(torch.unique(support_labels))\n","        # Prototype i is the mean of all instances of features corresponding to labels == i\n","        z_proto = torch.cat(\n","            [\n","                z_support[torch.nonzero(support_labels == label)].mean(0)\n","                for label in range(n_way)\n","            ]\n","        )\n","\n","        # Compute the euclidean distance from queries to prototypes\n","        dists = torch.cdist(z_query, z_proto)\n","\n","        # And here is the super complicated operation to transform those distances into classification scores!\n","        scores = -dists\n","        return scores\n","\n","\n","convolutional_network = torchvision.models.resnet50(pretrained=True)\n","convolutional_network.fc = nn.Flatten()\n","#print(convolutional_network)\n","\n","model = PrototypicalNetworks(convolutional_network).cuda()\n","print(model)\n"]},{"cell_type":"markdown","metadata":{"id":"JnWfQfmZ7Krs","pycharm":{"name":"#%% md\n"}},"source":["Now we have a model! Note that we used a pretrained feature extractor,\n","so our model should already be up and running. Let's see that.\n","\n","Here we create a dataloader that will feed few-shot classification tasks to our model.\n","But a regular PyTorch dataloader will feed batches of images, with no consideration for\n","their label or whether they are support or query. We need 2 specific features in our case.\n","\n","1. We need images evenly distributed between a given number of classes.\n","2. We need them split between support and query sets.\n","\n","For the first point, I wrote a custom sampler: it first samples `n_way` classes from the dataset,\n","then it samples `n_shot + n_query` images for each class (for a total of `n_way * (n_shot + n_query)`\n","images in each batch).\n","For the second point, I have a custom collate function to replace the built-in PyTorch `collate_fn`.\n","This baby feed each batch as the combination of 5 items:\n","\n","1. support images\n","2. support labels between 0 and `n_way`\n","3. query images\n","4. query labels between 0 and `n_way`\n","5. a mapping of each label in `range(n_way)` to its true class id in the dataset\n","(it's not used by the model but it's very useful for us to know what the true class is)\n","\n","You can see that in PyTorch, a DataLoader is basically the combination of a sampler, a dataset and a collate function\n","(and some multiprocessing voodoo): sampler says which items to fetch, the dataset says how to fetch them, and\n","the collate function says how to present these items together. If you want to dive into these custom objects,\n","they're [here](https://github.com/sicara/easy-few-shot-learning/tree/master/easyfsl/data_tools)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:06:13.111638Z","iopub.status.busy":"2022-12-04T19:06:13.110908Z","iopub.status.idle":"2022-12-04T19:06:13.149686Z","shell.execute_reply":"2022-12-04T19:06:13.147703Z","shell.execute_reply.started":"2022-12-04T19:06:13.111595Z"},"id":"OyS0-oRV7Krt","outputId":"7b72a127-4b81-4781-8bed-07a0e54e95db","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["N_WAY = 5  # Number of classes in a task\n","N_SHOT = 5  # Number of images per class in the support set\n","N_QUERY = 10  # Number of images per class in the query set\n","N_EVALUATION_TASKS = 100\n","\n","test_set.get_labels = lambda: test_set._labels\n","\n","test_sampler = TaskSampler(\n","    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",")\n","\n","test_loader = DataLoader(\n","    test_set,\n","    batch_sampler=test_sampler,\n","    num_workers=12,\n","    pin_memory=True,\n","    collate_fn=test_sampler.episodic_collate_fn,\n",")"]},{"cell_type":"markdown","metadata":{"id":"3gseUp_J7Krt","pycharm":{"name":"#%% md\n"}},"source":["We created a dataloader that will feed us with 5-way 5-shot tasks (the most common setting in the litterature).\n","Now, as every data scientist should do before launching opaque training scripts,\n","let's take a look at our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:06:13.151815Z","iopub.status.busy":"2022-12-04T19:06:13.151465Z","iopub.status.idle":"2022-12-04T19:06:31.273733Z","shell.execute_reply":"2022-12-04T19:06:31.272595Z","shell.execute_reply.started":"2022-12-04T19:06:13.151781Z"},"id":"_FSj8NIr7Krt","outputId":"a8f63876-f1c4-4d9b-821b-ffe84baf0282","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["(\n","    example_support_images,\n","    example_support_labels,\n","    example_query_images,\n","    example_query_labels,\n","    example_class_ids,\n",") = next(iter(test_loader))\n","\n","print(example_class_ids)\n","print(test_set.idx_to_class[i] for i in list(example_class_ids))\n","\n","plot_images(example_support_images, \"support images\", images_per_row=N_SHOT)\n","plot_images(example_query_images, \"query images\", images_per_row=N_QUERY)"]},{"cell_type":"markdown","metadata":{"id":"MNdmSpw87Kru","pycharm":{"name":"#%% md\n"}},"source":["For both support and query set, you should have one line for each class.\n","\n","How does our model perform on this task?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:06:31.280176Z","iopub.status.busy":"2022-12-04T19:06:31.275361Z","iopub.status.idle":"2022-12-04T19:06:42.653401Z","shell.execute_reply":"2022-12-04T19:06:42.652062Z","shell.execute_reply.started":"2022-12-04T19:06:31.28014Z"},"id":"C2EhF2Fa7Kru","outputId":"6443de04-c974-4209-c634-0377aa5fcd77","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["model.eval()\n","example_scores = model(\n","    example_support_images.cuda(),\n","    example_support_labels.cuda(),\n","    example_query_images.cuda(),\n",").detach()\n","\n","_, example_predicted_labels = torch.max(example_scores.data, 1)\n","\n","##\n","# print(example_scores.shape)\n","# print(torch.softmax(example_scores, dim=1) )\n","\n","##\n","print(\"Some example predictions on query images:\")\n","print(\"Ground Truth - Predicted\")\n","print(\"________________________\")\n","for i in range(len(example_query_labels)):\n","    print(\n","        # f\"{test_set._characters[example_class_ids[example_query_labels[i]]]} / {test_set._characters[example_class_ids[example_predicted_labels[i]]]}\"\n","        f\"{test_set.idx_to_class[example_class_ids[example_query_labels[i]]]} - {test_set.idx_to_class[example_class_ids[example_predicted_labels[i]]]}\"\n","    )"]},{"cell_type":"markdown","metadata":{"id":"TznXhcUL7Kru","pycharm":{"name":"#%% md\n"}},"source":["This doesn't look bad: keep in mind that the model was trained on very different images, and has only seen 5 examples for each class!\n","\n","Now that we have a first idea, let's see more precisely how good our model is."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:09:41.262302Z","iopub.status.busy":"2022-12-04T19:09:41.261871Z","iopub.status.idle":"2022-12-04T19:10:38.934275Z","shell.execute_reply":"2022-12-04T19:10:38.933077Z","shell.execute_reply.started":"2022-12-04T19:09:41.262264Z"},"id":"UW5Rxifk7Kru","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["def evaluate_on_one_task(\n","    support_images: torch.Tensor,\n","    support_labels: torch.Tensor,\n","    query_images: torch.Tensor,\n",") -> [int, int]:\n","    \"\"\"\n","    Returns the number of correct predictions of query labels, and the total number of predictions.\n","    \"\"\"\n","    \n","    y_pred = torch.max(\n","            model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n","            .detach()\n","            .data,\n","            1,\n","        )[1]\n","    return y_pred\n","\n","def evaluate(data_loader: DataLoader):\n","    # We'll count everything and compute the ratio at the end\n","    total_predictions = 0\n","    correct_predictions = 0\n","    f1_scores = []\n","\n","    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n","    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n","    model.eval()\n","    with torch.no_grad():\n","        for episode_index, (\n","            support_images,\n","            support_labels,\n","            query_images,\n","            query_labels,\n","            class_ids,\n","        ) in enumerate(data_loader):\n","\n","            y_pred = evaluate_on_one_task(\n","                support_images, support_labels, query_images,\n","            )\n","            total_predictions += len(query_labels)\n","            correct_predictions += (y_pred == query_labels.cuda()).sum().item()\n","            f1_scores.append(f1_score(query_labels.cpu(), y_pred.cpu(), average='macro')) \n","\n","    accuracy = (correct_predictions/total_predictions)\n","    avg_f1 = sum(f1_scores)/len(f1_scores)\n","    print(\n","        f\"Model tested on {len(data_loader)} tasks. Accuracy: {accuracy:.2%}, F1: {avg_f1}\"\n","    )\n","    \n","    return accuracy\n","\n","\n","# evaluate(test_loader)"]},{"cell_type":"markdown","metadata":{"id":"3dMqML2U7Krv","pycharm":{"name":"#%% md\n"}},"source":["With absolutely zero training on Omniglot images, and only 5 examples per class, we achieve around 86% accuracy! Isn't this a great start?\n","\n","Now that you know how to make Prototypical Networks work, you can see what happens if you tweak it\n","a little bit (change the backbone, use other distances than euclidean...) or if you change the problem\n","(more classes in each task, less or more examples in the support set, maybe even one example only,\n","but keep in mind that in that case Prototypical Networks are just standard nearest neighbour).\n","\n","When you're done, you can scroll further down and learn how to **meta-train this model**, to get even better results."]},{"cell_type":"markdown","metadata":{"id":"pLCdAgKTlpFU","pycharm":{"name":"#%% md\n"}},"source":["## Training a meta-learning algorithm\n","\n","Let's use the \"background\" images of Omniglot as training set. Here we prepare a data loader of 40 000 few-shot classification\n","tasks on which we will train our model. The alphabets used in the training set are entirely separated from those used in the testing set.\n","This guarantees that at test time, the model will have to classify characters that were not seen during training.\n","\n","Note that we don't set a validation set here to keep this notebook concise,\n","but keep in mind that **this is not good practice** and you should always use validation when training a model for production."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:11:50.494488Z","iopub.status.busy":"2022-12-04T19:11:50.494055Z","iopub.status.idle":"2022-12-04T19:11:50.527871Z","shell.execute_reply":"2022-12-04T19:11:50.526592Z","shell.execute_reply.started":"2022-12-04T19:11:50.49445Z"},"id":"YW9DDxbl7Krv","outputId":"e237a732-fe67-4294-8c95-67062b0466af","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["N_TRAINING_EPISODES = 60000\n","N_VALIDATION_TASKS = 100\n","\n","train_sampler = TaskSampler(\n","    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n",")\n","train_loader = DataLoader(\n","    train_set,\n","    batch_sampler=train_sampler,\n","    num_workers=12,\n","    pin_memory=True,\n","    collate_fn=train_sampler.episodic_collate_fn,\n",")\n","\n","val_sampler = TaskSampler(\n","    val_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_VALIDATION_TASKS\n",")\n","val_loader = DataLoader(\n","    val_set,\n","    batch_sampler=val_sampler,\n","    num_workers=12,\n","    pin_memory=True,\n","    collate_fn=val_sampler.episodic_collate_fn,\n",")"]},{"cell_type":"markdown","metadata":{"id":"QK5UN51alpFV","pycharm":{"name":"#%% md\n"}},"source":["We will keep the same model. So our weights will be pre-trained on ImageNet. If you want to start a training from scratch,\n","feel free to set `pretrained=False` in the definition of the ResNet.\n","\n","Here we define our loss and our optimizer (cross entropy and Adam, pretty standard), and a `fit` method.\n","This method takes a classification task as input (support set and query set). It predicts the labels of the query set\n","based on the information from the support set; then it compares the predicted labels to ground truth query labels,\n","and this gives us a loss value. Then it uses this loss to update the parameters of the model. This is a *meta-training loop*."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:15:02.206404Z","iopub.status.busy":"2022-12-04T19:15:02.205982Z","iopub.status.idle":"2022-12-04T19:15:02.214704Z","shell.execute_reply":"2022-12-04T19:15:02.213563Z","shell.execute_reply.started":"2022-12-04T19:15:02.206369Z"},"id":"0B1xX1Cb7Krv","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","def fit(\n","    support_images: torch.Tensor,\n","    support_labels: torch.Tensor,\n","    query_images: torch.Tensor,\n","    query_labels: torch.Tensor,\n",") -> float:\n","    optimizer.zero_grad()\n","    classification_scores = model(\n","        support_images.cuda(), support_labels.cuda(), query_images.cuda()\n","    )\n","\n","    loss = criterion(classification_scores, query_labels.cuda())\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()"]},{"cell_type":"markdown","metadata":{"id":"TUFYG6PElpFV","pycharm":{"name":"#%% md\n"}},"source":["To train the model, we are just going to iterate over a large number of randomly generated few-shot classification tasks,\n","and let the `fit` method update our model after each task. This is called **episodic training**.\n","\n","This took me 20mn on an RTX 2080 and I promised you that this whole tutorial would take 15mn.\n","So if you don't want to run the training yourself, you can just skip the training and load the model that I trained\n","using the exact same code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:15:06.137456Z","iopub.status.busy":"2022-12-04T19:15:06.137075Z","iopub.status.idle":"2022-12-04T19:22:46.399889Z","shell.execute_reply":"2022-12-04T19:22:46.395676Z","shell.execute_reply.started":"2022-12-04T19:15:06.137424Z"},"id":"xQyS6uck7Krv","outputId":"ab3fe0db-aee9-42bd-e428-99ad1e7c354e","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["log_update_frequency = 100\n","acc_update_frequency = 500\n","\n","\n","all_loss = []\n","all_acc = []\n","\n","model.train()\n","with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n","    \n","    for episode_index, (\n","        support_images,\n","        support_labels,\n","        query_images,\n","        query_labels,\n","        _,\n","    ) in tqdm_train:\n","        loss_value = fit(support_images, support_labels, query_images, query_labels)\n","        all_loss.append(loss_value)\n","        \n","        evaluate(val_loader)\n","\n","        if episode_index % log_update_frequency == 0:\n","            tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n","        # if episode_index % acc_update_frequency == 0:\n","        #     all_acc.append(evaluate(test_loader))\n","        #     model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-04T19:25:13.923927Z","iopub.status.busy":"2022-12-04T19:25:13.923539Z","iopub.status.idle":"2022-12-04T19:25:14.138584Z","shell.execute_reply":"2022-12-04T19:25:14.137461Z","shell.execute_reply.started":"2022-12-04T19:25:13.923895Z"},"trusted":true},"outputs":[],"source":["window_width = 50\n","cumsum_vec = np.cumsum(np.insert(all_loss, 0, 0)) \n","ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n","\n","plt.title(\"Loss vs # Training Episodes\")\n","plt.xlabel(\"# Training Episodes\")\n","plt.ylabel(\"Loss\")\n","plt.plot(ma_vec, color = \"green\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-04T19:06:58.126924Z","iopub.status.idle":"2022-12-04T19:06:58.127774Z","shell.execute_reply":"2022-12-04T19:06:58.127548Z","shell.execute_reply.started":"2022-12-04T19:06:58.127507Z"},"id":"9bmPWd-8lpFW","outputId":"64884fde-94db-46d5-ace0-ff4eb1f2bb46","pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["evaluate(test_loader)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30301,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
